import os
import streamlit as st
from openai import OpenAI
from moviepy.editor import (
    AudioFileClip,
    CompositeVideoClip,
    ColorClip,
    ImageClip,
)
import numpy as np
from PIL import Image, ImageDraw, ImageFont

# =========================
# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# =========================
api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

if not api_key:
    st.error(
        "OPENAI_API_KEY ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n"
        "- Streamlit Cloudì˜ 'Edit secrets'ì—ì„œ\n"
        '  OPENAI_API_KEY = "sk-..." í˜•ì‹ìœ¼ë¡œ ì¶”ê°€í•´ ì£¼ì„¸ìš”.'
    )
    st.stop()

client = OpenAI(api_key=api_key)

# í°íŠ¸ (ë ˆí¬ ë£¨íŠ¸ì— NanumGothic.ttf ê°€ ìˆë‹¤ê³  ê°€ì •)
FONT_PATH = os.path.join(os.path.dirname(__file__), "NanumGothic.ttf")


# ====================================
# 0) Pillowë¡œ ìë§‰ ì´ë¯¸ì§€ë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜
# ====================================
def make_subtitle_image(
    text: str,
    width: int,
    font_size: int = 70,
    font_path: str | None = None,
    text_color=(255, 255, 255),
    outline_color=(0, 0, 0),
    outline_width: int = 3,
):
    """
    Pillowë¥¼ ì´ìš©í•´ ìë§‰ìš© í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±.
    í­(width)ì— ë§ê²Œ ìë™ ì¤„ë°”ê¿ˆí•˜ê³ , ì¤‘ì•™ ì •ë ¬.
    """
    if not text:
        text = " "

    # í°íŠ¸ ë¡œë“œ
    try:
        if font_path and os.path.isfile(font_path):
            font = ImageFont.truetype(font_path, font_size)
        else:
            font = ImageFont.truetype("arial.ttf", font_size)
    except Exception:
        font = ImageFont.load_default()

    # ë¨¼ì € í° ìº”ë²„ìŠ¤ì— ê·¸ë ¤ì„œ ë†’ì´ ê³„ì‚°
    dummy_img = Image.new("RGBA", (width, font_size * 4), (0, 0, 0, 0))
    draw = ImageDraw.Draw(dummy_img)

    # ê°„ë‹¨í•œ ìˆ˜ë™ ì¤„ë°”ê¿ˆ
    words = text.split(" ")
    lines = []
    current_line = ""
    for w in words:
        trial = (current_line + " " + w).strip()
        bbox = draw.textbbox((0, 0), trial, font=font)
        line_width = bbox[2] - bbox[0]
        if line_width <= width:
            current_line = trial
        else:
            if current_line:
                lines.append(current_line)
            current_line = w
    if current_line:
        lines.append(current_line)

    # ì „ì²´ ë†’ì´ ê³„ì‚°
    line_height = font_size + 8
    img_height = line_height * len(lines)

    img = Image.new("RGBA", (width, img_height), (0, 0, 0, 0))
    draw = ImageDraw.Draw(img)

    y = 0
    for line in lines:
        bbox = draw.textbbox((0, 0), line, font=font)
        line_width = bbox[2] - bbox[0]

        x = (width - line_width) // 2

        # ì™¸ê³½ì„ 
        if outline_width > 0:
            for dx in range(-outline_width, outline_width + 1):
                for dy in range(-outline_width, outline_width + 1):
                    if dx == 0 and dy == 0:
                        continue
                    draw.text((x + dx, y + dy), line, font=font, fill=outline_color)

        # ë³¸ í…ìŠ¤íŠ¸
        draw.text((x, y), line, font=font, fill=text_color)

        y += line_height

    return img


# ====================================
# 1) í…ìŠ¤íŠ¸ -> ìŒì„± (OpenAI TTS)
# ====================================
def generate_tts(text: str, output_path: str = "tts_audio.mp3") -> str:
    """
    í…ìŠ¤íŠ¸ë¥¼ OpenAI TTSë¡œ mp3 íŒŒì¼ë¡œ ì €ì¥.
    """
    response = client.audio.speech.create(
        model="gpt-4o-mini-tts",
        voice="alloy",
        input=text,
    )

    audio_bytes = response.read()

    with open(output_path, "wb") as f:
        f.write(audio_bytes)

    return output_path


# ====================================
# 2) ìŒì„± -> íƒ€ì„ìŠ¤íƒ¬í”„ (Whisper)
# ====================================
def extract_word_timestamps(audio_path: str):
    """
    Whisper(whisper-1)ë¡œ ë‹¨ì–´ ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ.
    """
    with open(audio_path, "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="verbose_json",
            timestamp_granularities=["word"],
        )

    words = getattr(transcript, "words", None)
    if words is None and isinstance(transcript, dict):
        words = transcript.get("words", [])

    if words is None:
        words = []

    return words


# ====================================
# 3) íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ìë§‰ + ë°°ê²½ í´ë¦½ ìƒì„±
# ====================================
def build_video_clips_from_words(words, video_size=(1080, 1920)):
    """
    Whisper ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ìë§‰ ì´ë¯¸ì§€ í´ë¦½ + ë°°ê²½ í´ë¦½ ìƒì„±.
    """
    W, H = video_size
    clips = []

    if not words:
        return clips, 0.0

    last_end = max((w.end if hasattr(w, "end") else w["end"]) for w in words)

    # ë°°ê²½(ê²€ì • í™”ë©´)
    bg = ColorClip(size=(W, H), color=(0, 0, 0), duration=last_end)
    clips.append(bg)

    for w in words:
        if hasattr(w, "word"):
            txt = w.word
            start = w.start
            end = w.end
        else:
            txt = w["word"]
            start = w["start"]
            end = w["end"]

        if end <= start:
            continue

        duration = end - start

        # Pillowë¡œ ìë§‰ ì´ë¯¸ì§€ ìƒì„±
        img = make_subtitle_image(
            txt,
            width=W - 200,
            font_size=70,
            font_path=FONT_PATH if os.path.isfile(FONT_PATH) else None,
        )

        img_array = np.array(img)
        text_clip = (
            ImageClip(img_array)
            .set_duration(duration)
            .set_start(start)
            .set_position(("center", H - 300))
        )

        clips.append(text_clip)

    return clips, last_end


# ====================================
# 4) ìŒì„± + ìë§‰ -> mp4 ì˜ìƒ ë§Œë“¤ê¸°
# ====================================
def create_video_with_subtitles(
    audio_path: str, words, output_path: str = "subking_result.mp4"
):
    clips, duration = build_video_clips_from_words(words)
    if duration <= 0:
        return None

    video = CompositeVideoClip(clips)
    audio = AudioFileClip(audio_path)
    video = video.set_audio(audio)

    video.write_videofile(
        output_path,
        fps=30,
        codec="libx264",
        audio_codec="aac",
        verbose=False,
        logger=None,
    )

    return output_path


# ====================================
# 5) Streamlit UI
# ====================================
st.set_page_config(page_title="SubKing", page_icon="ğŸ¬", layout="centered")

st.title("ğŸ¬ SubKing - í…ìŠ¤íŠ¸ë¡œ ìŒì„± + ìë§‰ ì˜ìƒ ë§Œë“¤ê¸°")

script = st.text_area(
    "ëŒ€ë³¸ì„ ì…ë ¥í•˜ì„¸ìš”",
    height=250,
    placeholder="ì—¬ê¸°ì— ì½ì–´ ì¤„ ë¬¸ì¥ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.",
)

if st.button("ğŸ¤ ìŒì„± + ìë§‰ ì˜ìƒ ìƒì„±"):
    if not script.strip():
        st.error("ëŒ€ë³¸ì„ ë¨¼ì € ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    with st.status("TTS ìƒì„± ì¤‘...", expanded=True) as status:
        # 1) ìŒì„± ìƒì„±
        audio_path = generate_tts(script)
        status.update(label="íƒ€ì„ìŠ¤íƒ¬í”„ ë¶„ì„ ì¤‘ (Whisper)...", state="running")

        # 2) íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
        words = extract_word_timestamps(audio_path)
        if not words:
            status.update(
                label="íƒ€ì„ìŠ¤íƒ¬í”„ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.",
                state="error",
            )
            st.stop()

        status.update(label="ì˜ìƒ ë Œë”ë§ ì¤‘ (MoviePy)...", state="running")

        # 3) ì˜ìƒ ìƒì„±
        video_path = create_video_with_subtitles(audio_path, words)

        if not video_path:
            status.update(label="ì˜ìƒ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", state="error")
            st.stop()

        status.update(label="ì™„ë£Œ! ğŸ‰", state="complete")

    st.success("ì˜ìƒì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    st.video(video_path)

    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    with open(video_path, "rb") as f:
        st.download_button(
            "ğŸ“¥ ì˜ìƒ ë‹¤ìš´ë¡œë“œ",
            f,
            file_name="subking_result.mp4",
            mime="video/mp4",
        )
